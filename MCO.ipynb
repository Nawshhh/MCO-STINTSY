{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427f731e",
   "metadata": {},
   "source": [
    "# STINTSY MCO\n",
    "The Major Course Output for STINTSY (Advanced Intelligent Systems) will include 11 sections. The following sections are:\n",
    "- **Section 1** : Introduction to the problem/task and dataset\n",
    "- **Section 2** : Description of the dataset\n",
    "- **Section 3** : List of requirements\n",
    "- **Section 4** : Data preprocessing and cleaning\n",
    "- **Section 5** : Exploratory data analysis\n",
    "- **Section 6** : Initial model training\n",
    "- **Section 7** : Error analysis\n",
    "- **Section 8** : Improving model performance\n",
    "- **Section 9** : Model Performance Summary\n",
    "- **Section 10** : Insights and conclusions\n",
    "- **Section 11** : References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbc32f",
   "metadata": {},
   "source": [
    "## Section 1 : Introduction \n",
    "\n",
    "Each group should select one real-world dataset from the list of datasets provided for the project. Each dataset is accompanied with a description file, which also contains detailed description of each feature.\n",
    "\n",
    "The target task (i.e., classification or regression) should be properly stated as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffe119",
   "metadata": {},
   "source": [
    "## Section 2 : Description of Dataset\n",
    "In this section of the notebook, you must fulfill the following:\n",
    "- State a brief description of the dataset.\n",
    "- Provide a description of the collection process executed to build the dataset. Discuss the implications of the data collection method on the generated conclusions and insights. Note that you may need to look at relevant sources related to the dataset to acquire necessary information for this part of the project.\n",
    "- Describe the structure of the dataset file.\n",
    "    - What does each row and column represent?\n",
    "    - How many instances are there in the dataset?\n",
    "    - How many features are there in the dataset?\n",
    "    - If the dataset is composed of different files that you will combine in the succeeding steps, describe the structure and the contents of each file.\n",
    "- Discuss the features in each dataset file. What does each feature represent? All features, even those which are not used for the study, should be described to the reader. The purpose of each feature in the dataset should be clear to the reader of the notebook without having to go through an external link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b6002",
   "metadata": {},
   "source": [
    "## Section 3 : List of Requirements\n",
    "List all the Python libraries and modules that you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e643a16-ffa3-4c49-b0d2-2c9f306b211d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0009b",
   "metadata": {},
   "source": [
    "## Section 4 : Data Preprocessing and Cleaning\n",
    "\n",
    "Perform necessary steps before using the data. In this section of the notebook, please take note of the following:\n",
    "\n",
    "- If needed, perform preprocessing techniques to transform the data to the appropriate representation. This may include binning, log transformations, conversion to one-hot encoding, normalization, standardization, interpolation, truncation, and feature engineering, among others. There should be a correct and proper justification for the use of each preprocessing technique used in the project.\n",
    "- Make sure that the data is clean, especially features that are used in the project. This may include checking for misrepresentations, checking the data type, dealing with missing data, dealing with duplicate data, and dealing with outliers, among others. There should be a correct and proper justification for the application (or non-application) of each data cleaning method used in the project. Clean only the variables utilized in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03faae1e-d5ce-4bd8-98fd-017bae2b7d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "filename = \"dataset.csv\"\n",
    "\n",
    "columns = [\n",
    "    \"W_REGN\", \"W_OID\", \"W_SHSN\", \"W_HCN\", \"URB\", \"RSTR\", \"PSU\", \"BWEIGHT\", \"RFACT\", \"FSIZE\",\n",
    "    \"AGRI_SAL\", \"NONAGRI_SAL\", \"WAGES\", \"NETSHARE\", \"CASH_ABROAD\", \"CASH_DOMESTIC\", \"RENTALS_REC\", \"INTEREST\",\n",
    "    \"PENSION\", \"DIVIDENDS\", \"OTHER_SOURCE\", \"NET_RECEIPT\", \"REGFT\", \"NET_CFG\", \"NET_LPR\", \"NET_FISH\", \"NET_FOR\",\n",
    "    \"NET_RET\", \"NET_MFG\", \"NET_COM\", \"NET_TRANS\", \"NET_MIN\", \"NET_CONS\", \"NET_NEC\", \"EAINC\", \"TOINC\", \"LOSSES\",\n",
    "    \"T_BREAD\", \"T_MEAT\", \"T_FISH\", \"T_MILK\", \"T_OIL\", \"T_FRUIT\", \"T_VEG\", \"T_SUGAR\", \"T_FOOD_NEC\", \"T_COFFEE\",\n",
    "    \"T_MINERAL\", \"T_ALCOHOL\", \"T_TOBACCO\", \"T_OTHER_VEG\", \"T_FOOD_HOME\", \"T_FOOD_OUTSIDE\", \"T_FOOD\", \"T_CLOTH\",\n",
    "    \"T_FURNISHING\", \"T_HEALTH\", \"T_HOUSING_WATER\", \"T_ACTRENT\", \"T_RENTVAL\", \"T_IMPUTED_RENT\", \"T_BIMPUTED_RENT\",\n",
    "    \"T_TRANSPORT\", \"T_COMMUNICATION\", \"T_RECREATION\", \"T_EDUCATION\", \"T_MISCELLANEOUS\", \"T_OTHER_EXPENDITURE\",\n",
    "    \"T_OTHER_DISBURSEMENT\", \"T_NFOOD\", \"T_TOTEX\", \"T_TOTDIS\", \"T_OTHREC\", \"T_TOREC\", \"FOOD_ACCOM_SRVC\", \"SEX\",\n",
    "    \"AGE\", \"MS\", \"HGC\", \"JOB\", \"OCCUP\", \"KB\", \"CW\", \"HHTYPE\", \"MEMBERS\", \"AGELESS5\", \"AGE5_17\", \"EMPLOYED_PAY\",\n",
    "    \"EMPLOYED_PROF\", \"SPOUSE_EMP\", \"BLDG_TYPE\", \"ROOF\", \"WALLS\", \"TENURE\", \"HSE_ALTERTN\", \"TOILET\", \"ELECTRIC\",\n",
    "    \"WATER\", \"DISTANCE\", \"RADIO_QTY\", \"TV_QTY\", \"CD_QTY\", \"STEREO_QTY\", \"REF_QTY\", \"WASH_QTY\", \"AIRCON_QTY\",\n",
    "    \"CAR_QTY\", \"LANDLINE_QTY\", \"CELLPHONE_QTY\", \"PC_QTY\", \"OVEN_QTY\", \"MOTOR_BANCA_QTY\", \"MOTORCYCLE_QTY\",\n",
    "    \"POP_ADJ\", \"PCINC\", \"NATPC\", \"NATDC\", \"REGDC\", \"REGPC\"\n",
    "]\n",
    "\n",
    "# int and float columns\n",
    "int_cols = [col for col in columns if col not in [\"BWEIGHT\", \"RFACT\", \"FSIZE\", \"POP_ADJ\", \"PCINC\"]]\n",
    "float_cols = [\"BWEIGHT\", \"RFACT\", \"FSIZE\", \"POP_ADJ\", \"PCINC\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "# reading the csv file\n",
    "with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        processed_row = []\n",
    "        for col in columns:\n",
    "            value = row[col].strip() if col in row and row[col] else \"\"\n",
    "            if col in int_cols:\n",
    "                processed_row.append(int(value) if value.isdigit() else None)  # handle missing values as None\n",
    "            elif col in float_cols:\n",
    "                try:\n",
    "                    processed_row.append(float(value))  # convert float columns\n",
    "                except ValueError:\n",
    "                    processed_row.append(np.nan)  # handle missing values as NaN\n",
    "        data.append(processed_row)\n",
    "\n",
    "# Convert to NumPy array\n",
    "raw_data = np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f3ae78-5c57-47e0-9f48-b477a1d900ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 101001000 2 25 2 21100 415052 138.25 200.6576 3.0 0 0 0 0 176000\n",
      "  16000 0 0 33000 0 0 4385 76666 0 0 0 0 0 0 0 0 0 0 0 0 325251 0 30263\n",
      "  29374 5204 3533 2136 2129 6517 1149 2472 1890 6356 0 0 0 91023 23330\n",
      "  114353 11191 3598 586 55128 0 19200 19200 0 17280 1470 49567 41200\n",
      "  18636 260 0 198916 313269 313269 0 325251 0 2 75 3 280 2 None None None\n",
      "  2 3 None 1 None None 3 1 1 1 1 2 1 1 1 None 1 1 1 1 1 1 None None None\n",
      "  2 1 1 None None 0.94617231 108417.0 9 8 8 9]\n",
      " [14 101001000 3 43 2 21100 415052 138.25 200.6576 12.5 0 0 0 0 0 14700 0\n",
      "  0 0 0 0 1350 369 0 0 0 0 355776 0 0 8000 0 0 0 363776 382895 0 97693\n",
      "  5820 28836 14484 3016 1230 7885 4145 1875 2532 9076 0 2450 0 176592\n",
      "  8925 185517 3335 4304 1645 24492 0 2700 2700 0 8826 3480 1136 1180\n",
      "  10260 0 0 61108 246625 246625 0 382895 0 1 48 2 250 1 1314 4741 3 2 13\n",
      "  2 4 None 1 2 1 5 5 1 2 3 1 1 None 1 1 1 2 None None None None None 3\n",
      "  None 1 None 1 0.94617231 30631.6 5 9 9 4]]\n"
     ]
    }
   ],
   "source": [
    "# print the first few rows of the array to verify (check dataset.csv)\n",
    "print(raw_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633fa17e-fe68-4202-a5b6-27a4fc3b8f93",
   "metadata": {},
   "source": [
    "### Dicitonaries\n",
    "This seciton of the notebook defined the dictionaries needed for classification algorithm/s.\n",
    "- W_REGN = W_REGN_dict\n",
    "- W_OID = W_OID_dict\n",
    "- URB_VS1 = URB_VS1_dict\n",
    "- NATPC_VS1 & NATDC_VS1 & REGDC_VS1 & REGPC_VS1 = RID_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a3797f0-c402-4532-a400-d428bd648b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_REGN_dict = {\n",
    "    13: \"Region XIII - NCR\",\n",
    "    14: \"Region XIV - CAR\",\n",
    "    1: \"Region I - Ilocos Region\",\n",
    "    2: \"Region II - Cagayan Valley\",\n",
    "    3: \"Region III - Central Luzon\",\n",
    "    41: \"Region IVa - Calabarzon\",\n",
    "    42: \"Region IVb - Mimaropa\",\n",
    "    5: \"Region V - Bicol Region\",\n",
    "    6: \"Region VI - Western Visayas\",\n",
    "    7: \"Region VII - Central Visayas\",\n",
    "    8: \"Region VIII - Eastern Visayas\",\n",
    "    9: \"Region IX - Western Mindanao\",\n",
    "    10: \"Region X - Northern Mindanao\",\n",
    "    11: \"Region XI - Southern Mindanao\",\n",
    "    12: \"Region XII - Central Mindanao\",\n",
    "    15: \"Region XV - ARMM\",\n",
    "    16: \"Region XVI - CARAGA\"\n",
    "}\n",
    "\n",
    "W_OID_dict = {\n",
    "    39: \"Manila\",\n",
    "    74: \"NCR-2nd Dist.\",\n",
    "    75: \"NCR-3rd Dist.\",\n",
    "    76: \"NCR-4th Dist.\",\n",
    "    1: \"Abra\",\n",
    "    27: \"Benguet\",\n",
    "    32: \"Ifugao\",\n",
    "    44: \"Kalinga\",\n",
    "    81: \"Mountain Province\",\n",
    "    28: \"Apayao\",\n",
    "    29: \"Ilocos Norte\",\n",
    "    33: \"Ilocos Sur\",\n",
    "    55: \"La Union\",\n",
    "    9: \"Pangasinan\",\n",
    "    15: \"Batanes\",\n",
    "    31: \"Cagayan\",\n",
    "    50: \"Isabela\",\n",
    "    57: \"Nueva Vizcaya\",\n",
    "    8: \"Quirino\",\n",
    "    49: \"Bataan\",\n",
    "    54: \"Bulacan\",\n",
    "    69: \"Nueva Ecija\",\n",
    "    77: \"Pampanga\",\n",
    "    10: \"Tarlac\",\n",
    "    21: \"Zambales\",\n",
    "    34: \"Aurora\",\n",
    "    56: \"Batangas\",\n",
    "    58: \"Cavite\",\n",
    "    40: \"Laguna\",\n",
    "    51: \"Quezon\",\n",
    "    52: \"Rizal\",\n",
    "    53: \"Marinduque\",\n",
    "    59: \"Occidental Mindoro\",\n",
    "    5: \"Oriental Mindoro\",\n",
    "    16: \"Palawan\",\n",
    "    17: \"Romblon\",\n",
    "    20: \"Albay\",\n",
    "    41: \"Camarines Norte\",\n",
    "    62: \"Camarines Sur\",\n",
    "    4: \"Catanduanes\",\n",
    "    6: \"Masbate\",\n",
    "    19: \"Sorsogon\",\n",
    "    30: \"Aklan\",\n",
    "    45: \"Antique\",\n",
    "    12: \"Capiz\",\n",
    "    79: \"Iloilo\",\n",
    "    22: \"Negros Occidental\",\n",
    "    46: \"Guimaras\",\n",
    "    61: \"Bohol\",\n",
    "    26: \"Negros Oriental\",\n",
    "    37: \"Siquijor\",\n",
    "    48: \"Eastern Samar\",\n",
    "    60: \"Leyte\",\n",
    "    64: \"Northern Samar\",\n",
    "    78: \"Samar (Western)\",\n",
    "    72: \"Southern Leyte\",\n",
    "    73: \"Biliran\",\n",
    "    83: \"Zamboanga del Norte\",\n",
    "    97: \"Zamboanga del Sur\",\n",
    "    13: \"Zamboanga Sibugay\",\n",
    "    18: \"Isabela City\",\n",
    "    35: \"Bukidnon\",\n",
    "    42: \"Camiguin\",\n",
    "    43: \"Lanao del Norte\",\n",
    "    23: \"Misamis Occidental\",\n",
    "    24: \"Misamis Oriental\",\n",
    "    25: \"Davao\",\n",
    "    82: \"Davao de Sur\",\n",
    "    47: \"Davao Oriental\",\n",
    "    63: \"Compostela Valley\",\n",
    "    65: \"Cotabato\",\n",
    "    80: \"South Cotabato\",\n",
    "    98: \"Sultan Kudarat\",\n",
    "    7: \"Sarangani\",\n",
    "    36: \"Cotabato City\",\n",
    "    38: \"Basilan\",\n",
    "    66: \"Lanao del Sur\",\n",
    "    70: \"Maguindanao\",\n",
    "    2: \"Sulu\",\n",
    "    67: \"Tawi-tawi\",\n",
    "    68: \"Agusan del Norte\",\n",
    "    3: \"Agusan del Sur\",\n",
    "    67: \"Surigao del Norte\",\n",
    "    68: \"Surigao del Sur\"\n",
    "}\n",
    "\n",
    "URB_VS1_dict = {\n",
    "    1 : \"Urban\",\n",
    "    2 : \"Rural\"\n",
    "}\n",
    "\n",
    "RID_dict = {\n",
    "    1: \"First Decile\",\n",
    "    2: \"Second Decile\",\n",
    "    3: \"Third Decile\",\n",
    "    4: \"Fourth Decile\",\n",
    "    5: \"Fifth Decile\",\n",
    "    6: \"Sixth Decile\",\n",
    "    7: \"Seventh Decile\",\n",
    "    8: \"Eighth Decile\",\n",
    "    9: \"Ninth Decile\",\n",
    "    10: \"Tenth Decile\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9f09a",
   "metadata": {},
   "source": [
    "## Section 5 : Exploratory Data Analysis\n",
    "\n",
    "Perform exploratory data analysis comprehensively to gain a good understanding of your dataset. In this section of the notebook, you must present relevant numerical summaries and visualizations. Make sure that each code is accompanied by a brief explanation. The whole process should be supported with verbose textual descriptions of your procedures and findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8dc8f2",
   "metadata": {},
   "source": [
    "## Section 6 : Initial Model Training\n",
    "Use machine learning models to accomplish your chosen task (i.e., classification or regression) for the dataset. In this section of the notebook, please take note of the following:\n",
    "- The project should train and evaluate <u> at least 3 different kinds</u> of machine learning models. The models should not be multiple variations of the same model, e.g., three neural network models with different number of neurons.\n",
    "- Each model should be appropriate in accomplishing the chosen task for the dataset. There should be a clear and correct justification on the use of each machine learning model.\n",
    "- Make sure that the values of the hyperparameters of each model are mentioned. At the minimum, the optimizer, the learning rate, and the learning rate schedule should be discussed per model.\n",
    "- The report should show that the models are not overfitting nor underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6474d2",
   "metadata": {},
   "source": [
    "### Section 6.1 : K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da13f58",
   "metadata": {},
   "source": [
    "### Section 6.2 : Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14fd20",
   "metadata": {},
   "source": [
    "### Section 6.3 : Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e77ca",
   "metadata": {},
   "source": [
    "## Section 7 : Error Analysis\n",
    "Perform error analysis on the output of all models used in the project. In this section of the notebook, you should:\n",
    "- Report and properly interpret the initial performance of all models using appropriate evaluation metrics.\n",
    "- Identify difficult classes and/or instances. For classification tasks, these are classes and/or instances that are difficult to classify. Hint: You may use confusion matrix for this. For regression tasks, these are instances that produces high error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23cc02",
   "metadata": {},
   "source": [
    "### Section 7.1 : Error Analysis for K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee53d6",
   "metadata": {},
   "source": [
    "### Section 7.2 : Error Analysis for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba75c51",
   "metadata": {},
   "source": [
    "### Section 7.3 : Error Analysis for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5831f",
   "metadata": {},
   "source": [
    "## Section 8 : Improving Model Performance\n",
    "Perform grid search or random search to tune the hyperparameters of each model. You should also tune each model to reduce the error in difficult classes and/or instances. In this section of the notebook, please take note of the following:\n",
    "- Make sure to elaborately explain the method of hyperparameter tuning.\n",
    "- Explicitly mention the different hyperparameters and their range of values. Show the corresponding performance of each configuration.\n",
    "- Report the performance of all models using appropriate evaluation metrics and visualizations.\n",
    "- Properly interpret the result based on relevant evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62adc38",
   "metadata": {},
   "source": [
    "### Section 8.1 : Improving K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb06a7",
   "metadata": {},
   "source": [
    "### Section 8.2 : Improving Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c19d2",
   "metadata": {},
   "source": [
    "### Section 8.3 : Improving Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a114d",
   "metadata": {},
   "source": [
    "## Section 9 : Model Performance Summary\n",
    "Present a summary of all model configurations. In this section of the notebook, do the following:\n",
    "- Discuss each algorithm and the best set of values for its hyperparameters. Identify the best model configuration and discuss its advantage over other configurations.\n",
    "- Discuss how tuning each model helped in reducing its error in difficult classes and/or instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9be70",
   "metadata": {},
   "source": [
    "## Section 10 : Insights and Conclusion\n",
    "Clearly state your insights and conclusions from training a model on the data. Why did some models produce better results? Summarize your conclusions to explain the performance of the models. Discuss recommendations to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979051a",
   "metadata": {},
   "source": [
    "## Section 11 : References\n",
    "Cite relevant references that you used in your project. All references must be cited, including:\n",
    "- Scholarly Articles – Cite in APA format and put a description of how you used it for your work.\n",
    "- Online references, blogs, articles that helped you come up with your project – Put the website, blog, or article title, link, and how you incorporated it into your work.\n",
    "- Artificial Intelligence (AI) Tools – Put the model used (e.g., ChatGPT, Gemini), the complete transcript of your conversations with the model (including your prompts and its responses), and a description of how you used it for your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18536c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
